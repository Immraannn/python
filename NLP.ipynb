{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b67cb3-3b7d-4fae-a0cc-91dab5e21723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in dataset: Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
      "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
      "      dtype='object')\n",
      "(525814, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "0                     1                       1  positive  1303862400   \n",
       "1                     0                       0  negative  1346976000   \n",
       "2                     1                       1  positive  1219017600   \n",
       "3                     3                       3  negative  1307923200   \n",
       "4                     0                       0  positive  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# First, extract the database file from the zip archive\n",
    "with zipfile.ZipFile('database.sqlite.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')  # Extract to current directory\n",
    "\n",
    "# Now connect to the extracted database file\n",
    "con = sqlite3.connect('database.sqlite')  # Connect to the extracted file, not the zip\n",
    "\n",
    "# Filter only positive and negative review\n",
    "# i.e., not taking 3 rating\n",
    "filtered_data = pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score !=3\"\"\", con)\n",
    "\n",
    "def partition(x):\n",
    "    # Function to convert rating score into sentiment label\n",
    "    # If score is less than 3 → 'negative', else → 'positive'\n",
    "    if x < 3:\n",
    "        return 'negative'\n",
    "    return 'positive'  # Fixed typo: 'positve' -> 'positive'\n",
    "\n",
    "# ✅ Print column names of dataset\n",
    "print(\"Column names in dataset:\", filtered_data.columns)\n",
    "\n",
    "# Convert original numeric score column into sentiment labels\n",
    "actualScore = filtered_data['Score']  # Extract numerical Score column\n",
    "\n",
    "# Apply partition() to each score value to map numbers to 'positive'/'negative'\n",
    "positiveNegative = actualScore.map(partition)\n",
    "\n",
    "# Replace original Score column with new sentiment labels\n",
    "filtered_data['Score'] = positiveNegative\n",
    "\n",
    "# Print dataset size (rows, columns)\n",
    "print(filtered_data.shape)\n",
    "\n",
    "# Display first few rows to check changes\n",
    "filtered_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f67efe9-5eca-42b3-a89a-2e2118848da1",
   "metadata": {},
   "source": [
    "<h2>#DATA CLEANING ;DEDULICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b8054a-3111-40f8-a695-7eb3659107f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78445</td>\n",
       "      <td>B000HDL1RQ</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138317</td>\n",
       "      <td>B000HDOPYC</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138277</td>\n",
       "      <td>B000HDOPYM</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73791</td>\n",
       "      <td>B000HDOPZG</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155049</td>\n",
       "      <td>B000PAQ75C</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId         UserId      ProfileName  HelpfulnessNumerator  \\\n",
       "0   78445  B000HDL1RQ  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "1  138317  B000HDOPYC  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "2  138277  B000HDOPYM  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "3   73791  B000HDOPZG  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "4  155049  B000PAQ75C  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time  \\\n",
       "0                       2      5  1199577600   \n",
       "1                       2      5  1199577600   \n",
       "2                       2      5  1199577600   \n",
       "3                       2      5  1199577600   \n",
       "4                       2      5  1199577600   \n",
       "\n",
       "                             Summary  \\\n",
       "0  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "1  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "2  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "3  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "4  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "\n",
       "                                                Text  \n",
       "0  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "1  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "2  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "3  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "4  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA CLEANING ;DEDULICATION\n",
    "#exaple of a id which is spamming the reiview there would be lot more id like this\n",
    "display=pd.read_sql_query(\"\"\"SELECT *FROM Reviews WHERE Score !=3 AND UserID=\"AR5J8UI46CURR\"ORDER BY ProductID\"\"\",con)\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3689549b-eb8f-4f0c-ae8b-872978400da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3143401a-e73e-429c-b37a-2e031e77098d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(525814, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138706</th>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>939340800</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138688</th>\n",
       "      <td>150506</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A2IW4PEEKO2R0U</td>\n",
       "      <td>Tracy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1194739200</td>\n",
       "      <td>Love the book, miss the hard cover version</td>\n",
       "      <td>I grew up reading these Sendak books, and watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138689</th>\n",
       "      <td>150507</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A1S4A3IQ2MU7V4</td>\n",
       "      <td>sally sue \"sally sue\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1191456000</td>\n",
       "      <td>chicken soup with rice months</td>\n",
       "      <td>This is a fun way for children to learn their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138690</th>\n",
       "      <td>150508</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AZGXZ2UUK6X</td>\n",
       "      <td>Catherine Hallberg \"(Kate)\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1076025600</td>\n",
       "      <td>a good swingy rhythm for reading aloud</td>\n",
       "      <td>This is a great little book to read aloud- it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138691</th>\n",
       "      <td>150509</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A3CMRKGE0P909G</td>\n",
       "      <td>Teresa</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>1018396800</td>\n",
       "      <td>A great way to learn the months</td>\n",
       "      <td>This is a book of poetry about the months of t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                  ProfileName  \\\n",
       "138706  150524  0006641040   ACITT7DI6IDDL              shari zychinski   \n",
       "138688  150506  0006641040  A2IW4PEEKO2R0U                        Tracy   \n",
       "138689  150507  0006641040  A1S4A3IQ2MU7V4        sally sue \"sally sue\"   \n",
       "138690  150508  0006641040     AZGXZ2UUK6X  Catherine Hallberg \"(Kate)\"   \n",
       "138691  150509  0006641040  A3CMRKGE0P909G                       Teresa   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "138706                     0                       0  positive   939340800   \n",
       "138688                     1                       1  positive  1194739200   \n",
       "138689                     1                       1  positive  1191456000   \n",
       "138690                     1                       1  positive  1076025600   \n",
       "138691                     3                       4  positive  1018396800   \n",
       "\n",
       "                                           Summary  \\\n",
       "138706                   EVERY book is educational   \n",
       "138688  Love the book, miss the hard cover version   \n",
       "138689               chicken soup with rice months   \n",
       "138690      a good swingy rhythm for reading aloud   \n",
       "138691             A great way to learn the months   \n",
       "\n",
       "                                                     Text  \n",
       "138706  this witty little book makes my son laugh at l...  \n",
       "138688  I grew up reading these Sendak books, and watc...  \n",
       "138689  This is a fun way for children to learn their ...  \n",
       "138690  This is a great little book to read aloud- it ...  \n",
       "138691  This is a book of poetry about the months of t...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorting data according to productid in ascending order\n",
    "# Sorting the dataset by 'ProductId' column\n",
    "# axis=0 → sort rows (default)\n",
    "# ascending=True → smallest → largest\n",
    "# inplace=False → do NOT modify original dataframe, return new sorted dataframe\n",
    "# kind='quicksort' → sorting algorithm (default, fastest for many cases)\n",
    "# na_position='last' → place missing values (NaN) at the end\n",
    "sorted_data = filtered_data.sort_values(\n",
    "    'ProductId',\n",
    "    axis=0,\n",
    "    ascending=True,\n",
    "    inplace=False,\n",
    "    kind='quicksort',\n",
    "    na_position='last'\n",
    ")\n",
    "\n",
    "# Print shape of sorted dataset (rows, columns)\n",
    "print(sorted_data.shape)\n",
    "sorted_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20000b1e-145f-4caf-b58f-8a1133cfa4dc",
   "metadata": {},
   "source": [
    "<h2>DEDUPLICATION OF ENTRIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf3263e3-6e11-4ad2-b1ae-64a44e849452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text']\n"
     ]
    }
   ],
   "source": [
    "#DEDUPLICATION OF ENTRIES\n",
    "final = sorted_data.drop_duplicates(\n",
    "    subset=[\"UserId\",\"ProfileName\",\"Time\",\"Text\"],  # columns used to check duplicates\n",
    "    keep='first',                                   # keep first occurrence\n",
    "    inplace=False                                   # return new dataframe instead of modifying original\n",
    ")\n",
    "\n",
    "final.shape  # shape after removing duplicates\n",
    "print(list(final.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b96644cf-818d-4a4d-af25-980da85dd9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.25890143662969"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CHECKING TO SEE HOW MUCH % OF DATA STILL REMAINS\n",
    "(final['Id'].size*1.0)/(filtered_data['Id'].size*1.0)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bef555e4-3bb5-4f29-a547-285ce0297526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>J. E. Stephens \"Jeanne\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>Bought This for My Son at College</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>Ram</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId              ProfileName  \\\n",
       "0  64422  B000MIDROQ  A161DK06JJMCYF  J. E. Stephens \"Jeanne\"   \n",
       "1  44737  B001EQ55RW  A2V0I904FH7ABY                      Ram   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     3                       1      5  1224892800   \n",
       "1                     3                       2      4  1212883200   \n",
       "\n",
       "                                        Summary  \\\n",
       "0             Bought This for My Son at College   \n",
       "1  Pure cocoa taste with crunchy almonds inside   \n",
       "\n",
       "                                                Text  \n",
       "0  My son loves spaghetti so I didn't hesitate or...  \n",
       "1  It was almost a 'love at first bite' - the per...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To remove those whose helpfulness numerator is> helpfulness denominaor¶\n",
    "display=pd.read_sql_query(\"\"\"SELECT *FROM Reviews WHERE Score !=3 AND Id=44737 OR Id=64422 ORDER BY ProductId\"\"\",con)\n",
    "print(display.shape)\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0db28fd-a366-4aca-99fa-3cff816c106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364171, 10)\n"
     ]
    }
   ],
   "source": [
    "final = final[\n",
    "    final.HelpfulnessNumerator <= final.HelpfulnessDenominator   # keep only rows where numerator is not greater than denominator\n",
    "]\n",
    "\n",
    "print(final.shape)  # print dataframe shape after removing invalid rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cca2131d-a0ea-4a85-ab0e-7bd6117d2d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text']\n"
     ]
    }
   ],
   "source": [
    "print(list(final.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3ec270e-df7e-4c6c-9454-1937841e0f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score\n",
       "positive    307061\n",
       "negative     57110\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HOW MANY POSITIVE AND NEGATIVE REVIEWS ARE PRESENT IN OUR DATASET?\n",
    "final['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15d1182a-9adf-4841-bbbc-b7fe9710c98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f17469cf-75cb-4f44-862f-dd0a091f6090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text']\n"
     ]
    }
   ],
   "source": [
    "print(list(final.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e25cb0-5cae-4ecc-ae4c-861d9d0554cb",
   "metadata": {},
   "source": [
    "<h2>TEXT PRERPOCESSING :STEMMING ,STOPWORD REMOVAL AND LEMITIZATION\n",
    "<h3>NOW WE HAVE FINISHED DEDUPLICATION OUR DATA REQUUIRES SOME PREPOCESSING BEFORE WE GO ON FURTHER WITH ANALYSIS AND \n",
    "<h3>MAKING THE PREDICTION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85d1e0b0-75cf-41bc-b54f-f72fc49a6731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'m', 'so', 'which', 'until', 'below', 'was', 'off', 'over', \"we're\", 'own', 'his', 'hasn', 'now', 'its', 'herself', \"mustn't\", 'had', 'have', \"should've\", 'under', \"doesn't\", \"shouldn't\", \"you've\", 'between', 'their', 'it', 'me', \"hadn't\", 'couldn', 'be', \"isn't\", 'll', 'him', 'doesn', 'down', 'or', 've', 'while', 'once', 'ain', 'we', 'haven', \"you'll\", 'themselves', 'i', 'yourself', 'too', \"mightn't\", \"they'll\", 'than', 'as', 'doing', 'mightn', 'up', \"you'd\", 's', 'wasn', \"she'd\", 'here', 'who', 'my', 'to', \"haven't\", 'a', 'that', 'wouldn', 'shouldn', 'he', 'aren', 'did', 'them', 'same', 'just', \"she'll\", \"weren't\", 'our', \"wasn't\", 'yourselves', \"it's\", 'no', \"wouldn't\", 'ourselves', 'you', 'before', 'because', 'this', 're', \"didn't\", 'there', 'are', 'being', 'each', \"i'll\", 'for', 'won', \"won't\", \"needn't\", 'if', 'the', \"shan't\", \"couldn't\", 'don', 'ma', 'has', 'in', 'what', \"they've\", 'not', 'ours', 'from', 'through', 'during', 'above', \"it'd\", 'didn', 't', 'and', \"i'd\", 'd', \"don't\", 'am', 'by', 'both', 'about', 'after', 'against', 'when', 'hers', \"he's\", 'any', 'out', \"they'd\", 'whom', 'why', 'such', 'weren', 'can', 'they', 'only', 'were', 'having', 'then', \"it'll\", 'shan', 'yours', 'at', 'an', 'been', \"i'm\", 'more', 'her', 'nor', \"hasn't\", \"she's\", 'how', 'these', \"he'd\", 'needn', 'those', \"you're\", 'other', 'is', \"we'll\", 'y', 'again', 'myself', \"aren't\", 'where', 'all', \"i've\", 'should', \"they're\", \"we've\", 'most', 'your', 'into', 'she', 'very', 'on', \"he'll\", 'with', 'few', 'hadn', 'itself', 'o', 'mustn', 'isn', \"that'll\", 'some', 'of', 'himself', 'do', 'theirs', \"we'd\", 'does', 'but', 'will', 'further'}\n",
      "***************************\n",
      "{'m', 'so', 'which', 'until', 'below', 'was', 'off', 'over', \"we're\", 'own', 'his', 'hasn', 'now', 'its', 'herself', \"mustn't\", 'had', 'have', \"should've\", 'under', \"doesn't\", \"shouldn't\", \"you've\", 'between', 'their', 'it', 'me', \"hadn't\", 'couldn', 'be', \"isn't\", 'll', 'him', 'doesn', 'down', 'or', 've', 'while', 'once', 'ain', 'we', 'haven', \"you'll\", 'themselves', 'i', 'yourself', 'too', \"mightn't\", \"they'll\", 'than', 'as', 'doing', 'mightn', 'up', \"you'd\", 's', 'wasn', \"she'd\", 'here', 'who', 'my', 'to', \"haven't\", 'a', 'that', 'wouldn', 'shouldn', 'he', 'aren', 'did', 'them', 'same', 'just', \"she'll\", \"weren't\", 'our', \"wasn't\", 'yourselves', \"it's\", 'no', \"wouldn't\", 'ourselves', 'you', 'before', 'because', 'this', 're', \"didn't\", 'there', 'are', 'being', 'each', \"i'll\", 'for', 'won', \"won't\", \"needn't\", 'if', 'the', \"shan't\", \"couldn't\", 'don', 'ma', 'has', 'in', 'what', \"they've\", 'not', 'ours', 'from', 'through', 'during', 'above', \"it'd\", 'didn', 't', 'and', \"i'd\", 'd', \"don't\", 'am', 'by', 'both', 'about', 'after', 'against', 'when', 'hers', \"he's\", 'any', 'out', \"they'd\", 'whom', 'why', 'such', 'weren', 'can', 'they', 'only', 'were', 'having', 'then', \"it'll\", 'shan', 'yours', 'at', 'an', 'been', \"i'm\", 'more', 'her', 'nor', \"hasn't\", \"she's\", 'how', 'these', \"he'd\", 'needn', 'those', \"you're\", 'other', 'is', \"we'll\", 'y', 'again', 'myself', \"aren't\", 'where', 'all', \"i've\", 'should', \"they're\", \"we've\", 'most', 'your', 'into', 'she', 'very', 'on', \"he'll\", 'with', 'few', 'hadn', 'itself', 'o', 'mustn', 'isn', \"that'll\", 'some', 'of', 'himself', 'do', 'theirs', \"we'd\", 'does', 'but', 'will', 'further'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re                                  # used for regex operations (cleaning text patterns like HTML tags)\n",
    "import nltk                                # NLP library for text preprocessing\n",
    "nltk.download('stopwords')                 # download stopwords list once (words like \"the\", \"is\", \"and\")\n",
    "import string                              # handles punctuation-related operations\n",
    "\n",
    "from nltk.corpus import stopwords          # import stopword dataset\n",
    "from nltk.stem import PorterStemmer        # stemming tool (reduces words to root form, e.g., running→run)\n",
    "from nltk.stem.wordnet import WordNetLemmatizer  # lemmatizer (smarter stemmer; grammar aware)\n",
    "\n",
    "# Creating a set of English stopwords (fast lookup)\n",
    "stop = set(stopwords.words('english'))\n",
    "print(stop)                                # print list of stopwords for reference\n",
    "\n",
    "# Initializing the Snowball stemmer (better than PorterStemmer in many cases)\n",
    "sno = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# FUNCTION: Remove HTML tags like <br>, <div>, <p>, etc.\n",
    "# Example: \"<br>great product</br>\" → \" great product \"\n",
    "# -------------------------------------------------------------------------\n",
    "def cleanhtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')           # regex pattern to match anything inside < >\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)  # replace HTML tags with a space\n",
    "    return cleantext                       # return cleaned sentence\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# FUNCTION: Remove punctuation marks\n",
    "# First line removes special punctuation like ? ! ' \" #\n",
    "# Second line replaces .,()|/ etc. with spaces (to avoid word merging)\n",
    "# Example: \"hello,world!\" → \"hello world\"\n",
    "# -------------------------------------------------------------------------\n",
    "def cleanpunc(sentence):\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]', r'', sentence)  # remove special punctuations completely\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]', r' ', cleaned) # replace other punctuation with space\n",
    "    return cleaned\n",
    "\n",
    "print('***************************')   \n",
    "print(stop)                                # print stopwords again to verify everything works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4da88d95-d320-4c1f-a5bb-458e53e99b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "<div><b>This product is <i>amazing</i>!</b> <br> I loved it.</div>\n",
      "\n",
      "After HTML cleaning:\n",
      "  This product is  amazing !    I loved it. \n"
     ]
    }
   ],
   "source": [
    "sample_text = \"<div><b>This product is <i>amazing</i>!</b> <br> I loved it.</div>\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nAfter HTML cleaning:\")\n",
    "print(cleanhtml(sample_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc4cc6e-250c-4448-90dc-045e36141de6",
   "metadata": {},
   "source": [
    "<h2>BAG OF WORDS(BOW)\n",
    "<h4>FINAL IS HERE FILTERED DATA AFTER DEDULPIATON AND HELPFULNESS NUM/DENOM OPERTION\n",
    "<h4>THSESE TWO WHERE DATA CLEANING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42a7ab9a-7de3-4456-b46b-08f18f459e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364171, 115281)\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Bag of Words (BOW) Feature Extraction --------------------\n",
    "\n",
    "import sklearn                                           # machine learning library\n",
    "from sklearn.feature_extraction.text import CountVectorizer   # tool to convert text → numeric vectors\n",
    "\n",
    "count_vect = CountVectorizer()                           # create BOW converter object (default settings)\n",
    "\n",
    "# fit() = learn vocabulary from text\n",
    "# transform() = convert each text review into numeric vector (word frequency)\n",
    "# fit_transform() = fit + transform in one step\n",
    "final_counts = count_vect.fit_transform(final['Text'].values)  \n",
    "\n",
    "# final_counts → sparse matrix (rows = reviews, columns = unique words)\n",
    "# Each cell stores frequency of word in that review\n",
    "\n",
    "print(final_counts.shape)                                # check shape (num_rows, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83592691-dc7a-4611-9718-872d661ce932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "095b4d89-f93f-4e57-b5ab-997b91123e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 115281)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_counts.get_shape()#here all column vector is unigram all unique words in unique col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80fc863b-58f7-44cb-8f37-920ea4628cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76e4eea2-4a8f-4fa4-926d-02564acd8b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 115281)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764c4990-0732-442e-9125-f5edfd173187",
   "metadata": {},
   "source": [
    "<h2>TEXT PREPOCESSING:STEMMING,STOP-REMOVAL,AND LEMMITIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44fbab1e-eb11-4d85-befa-aa535aa31c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "I set aside at least an hour each day to read to my son (3 y/o). At this point, I consider myself a connoisseur of children's books and this is one of the best. Santa Clause put this under the tree. Since then, we've read it perpetually and he loves it.<br /><br />First, this book taught him the months of the year.<br /><br />Second, it's a pleasure to read. Well suited to 1.5 y/o old to 4+.<br /><br />Very few children's books are worth owning. Most should be borrowed from the library. This book, however, deserves a permanent spot on your shelf. Sendak's best.\n"
     ]
    }
   ],
   "source": [
    "#FINDING SENTENCE USING HTML TAGS\n",
    "i = 0   # index counter\n",
    "\n",
    "# iterate through each review/text in the dataset\n",
    "for sent in final['Text'].values:     \n",
    "\n",
    "    # check if sentence still contains any HTML tags <...>\n",
    "    if(len(re.findall('<.*?>', sent))):     \n",
    "        print(i)        # print index of the review with HTML tag\n",
    "        print(sent)     # print the sentence containing HTML\n",
    "        break           # stop after finding first such sentence\n",
    "    \n",
    "    i += 1  # increase index counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04bfd718-b0c5-4cac-b267-7ac21429dcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'m', 'so', 'which', 'until', 'below', 'was', 'off', 'over', \"we're\", 'own', 'his', 'hasn', 'now', 'its', 'herself', \"mustn't\", 'had', 'have', \"should've\", 'under', \"doesn't\", \"shouldn't\", \"you've\", 'between', 'their', 'it', 'me', \"hadn't\", 'couldn', 'be', \"isn't\", 'll', 'him', 'doesn', 'down', 'or', 've', 'while', 'once', 'ain', 'we', 'haven', \"you'll\", 'themselves', 'i', 'yourself', 'too', \"mightn't\", \"they'll\", 'than', 'as', 'doing', 'mightn', 'up', \"you'd\", 's', 'wasn', \"she'd\", 'here', 'who', 'my', 'to', \"haven't\", 'a', 'that', 'wouldn', 'shouldn', 'he', 'aren', 'did', 'them', 'same', 'just', \"she'll\", \"weren't\", 'our', \"wasn't\", 'yourselves', \"it's\", 'no', \"wouldn't\", 'ourselves', 'you', 'before', 'because', 'this', 're', \"didn't\", 'there', 'are', 'being', 'each', \"i'll\", 'for', 'won', \"won't\", \"needn't\", 'if', 'the', \"shan't\", \"couldn't\", 'don', 'ma', 'has', 'in', 'what', \"they've\", 'not', 'ours', 'from', 'through', 'during', 'above', \"it'd\", 'didn', 't', 'and', \"i'd\", 'd', \"don't\", 'am', 'by', 'both', 'about', 'after', 'against', 'when', 'hers', \"he's\", 'any', 'out', \"they'd\", 'whom', 'why', 'such', 'weren', 'can', 'they', 'only', 'were', 'having', 'then', \"it'll\", 'shan', 'yours', 'at', 'an', 'been', \"i'm\", 'more', 'her', 'nor', \"hasn't\", \"she's\", 'how', 'these', \"he'd\", 'needn', 'those', \"you're\", 'other', 'is', \"we'll\", 'y', 'again', 'myself', \"aren't\", 'where', 'all', \"i've\", 'should', \"they're\", \"we've\", 'most', 'your', 'into', 'she', 'very', 'on', \"he'll\", 'with', 'few', 'hadn', 'itself', 'o', 'mustn', 'isn', \"that'll\", 'some', 'of', 'himself', 'do', 'theirs', \"we'd\", 'does', 'but', 'will', 'further'}\n",
      "*************************\n",
      "tasti\n"
     ]
    }
   ],
   "source": [
    "import re                                 # regex library for pattern matching (HTML removal)\n",
    "import nltk                               # natural language toolkit\n",
    "import string                             # handles punctuation-related operations\n",
    "\n",
    "from nltk.corpus import stopwords         # import stopword list (common useless words)\n",
    "from nltk.stem import PorterStemmer       # stemmer (not used here but imported)\n",
    "from nltk.stem.wordnet import WordNetLemmatizer  # lemmatizer (not used here but imported)\n",
    "\n",
    "# Create a set of English stopwords (faster lookup than list)\n",
    "stop = set(stopwords.words('english'))     # e.g., \"the\", \"is\", \"and\", \"are\"\n",
    "                                           # removing these helps model focus on meaningful words\n",
    "\n",
    "# Initialize Snowball Stemmer for English (better than PorterStemmer generally)\n",
    "sno = nltk.stem.SnowballStemmer('english')  # used for stemming words, e.g., tasty → tasti\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# FUNCTION: Remove HTML tags such as <br>, <div>, <p> etc.\n",
    "# Example: \"<br>good product</br>\" → \" good product \"\n",
    "# -----------------------------------------------------------------\n",
    "def cleanhtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')           # regex pattern to match anything inside < >\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)  # replace HTML tags with a space\n",
    "    return cleantext                       # return cleaned sentence\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# FUNCTION: Remove punctuation from text\n",
    "# First step removes specific punctuation ? ! ' \" #\n",
    "# Second step replaces . , ( ) | / with space so words don’t stick together\n",
    "# Example: \"hello,world!\" → \"hello world\"\n",
    "# -----------------------------------------------------------------\n",
    "def cleanpunc(sentence):\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]', r'', sentence)  # remove special punctuations\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]', r' ', cleaned) # replace other punctuation with a space\n",
    "    return cleaned\n",
    "\n",
    "# Print stopwords to verify\n",
    "print(stop)   \n",
    "print(\"*************************\")\n",
    "\n",
    "# Checking stemmer on an example\n",
    "print(sno.stem('tasty'))                   # output: \"tasti\" (stem of tasty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf443a09-73d7-4da8-a34d-56382b75d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Pre-processing Loop --------------------\n",
    "# This prepares cleaned/stemmed text and collects positive & negative words\n",
    "\n",
    "i = 0                              # index to track review score\n",
    "str1 = ' '                         # temporary string holder\n",
    "final_string = []                  # list to store cleaned reviews\n",
    "all_positive_words = []            # store words from positive reviews\n",
    "all_negative_words = []            # store words from negative reviews\n",
    "s = ''                             # temporary variable for each processed word\n",
    "\n",
    "# iterate over each review in the dataset\n",
    "for sent in final['Text'].values:\n",
    "    \n",
    "    filtered_sentence = []         # list for cleaned words in single review\n",
    "    \n",
    "    # remove HTML tags like <br>, <p>, etc.\n",
    "    sent = cleanhtml(sent)\n",
    "    \n",
    "    # split sentence into tokens/words\n",
    "    for w in sent.split():\n",
    "        \n",
    "        # remove punctuation & possibly split into further tokens\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            \n",
    "            # keep only alphabetic words (ignore numbers, symbols) AND length > 2\n",
    "            if cleaned_words.isalpha() and len(cleaned_words) > 2:\n",
    "                \n",
    "                # ignore stopwords (\"the\", \"is\", \"and\", ...)\n",
    "                if cleaned_words.lower() not in stop:\n",
    "                    \n",
    "                    # stemming: \"loving\" -> \"love\" ; lowercasing ; encode text\n",
    "                    s = sno.stem(cleaned_words.lower()).encode('utf8')\n",
    "                    \n",
    "                    # add processed word to review sentence\n",
    "                    filtered_sentence.append(s)\n",
    "                    \n",
    "                    # if this review has positive sentiment, collect its words\n",
    "                    if final['Score'].values[i] == 'positive':\n",
    "                        all_positive_words.append(s)\n",
    "                    \n",
    "                    # if this review has negative sentiment, collect its words\n",
    "                    if final['Score'].values[i] == 'negative':\n",
    "                        all_negative_words.append(s)\n",
    "                else:\n",
    "                    continue      # skip if stopword\n",
    "            else:\n",
    "                continue          # skip if not alphabetic or too short\n",
    "    \n",
    "    # join processed words back to sentence (byte-string format)\n",
    "    str1 = b\" \".join(filtered_sentence)\n",
    "    \n",
    "    # append cleaned review to final list\n",
    "    final_string.append(str1)\n",
    "    \n",
    "    # move to next review / label\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60b15749-e470-44a6-b537-4dc074563ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cleaned processed text (stored in final_string) as a new column\n",
    "# Each entry in 'final_string' corresponds to the cleaned version of review text\n",
    "final['CleanedText'] = final_string\n",
    "#What this does\n",
    "#creates a new column in your final DataFrame named CleanedText\n",
    "#Stores the cleaned/stemmed/processed review text in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5afe250a-1cb3-480d-87f5-83313c169e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 11)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28fdf583-e242-4876-bf05-8836e626b5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text', 'CleanedText']\n"
     ]
    }
   ],
   "source": [
    "print(list(final.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8fffccf-a883-4357-8407-6970ccf118f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364171"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first 3 rows of the final DataFrame \n",
    "# (to visually confirm that CleanedText column has processed reviews)\n",
    "final.head(3)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Store the cleaned & processed review table in SQLite database\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# Create/Connect to SQLite database file named 'final.sqlite'\n",
    "conn = sqlite3.connect('final.sqlite')    \n",
    "\n",
    "# Create a cursor object to execute SQL commands\n",
    "c = conn.cursor()                         \n",
    "\n",
    "# Ensure SQLite can store text data properly\n",
    "conn.text_factory = str                   \n",
    "\n",
    "# Save the DataFrame into SQLite DB\n",
    "# 'Reviews' will be the table name\n",
    "# if_exists='replace' → replace table if it already exists\n",
    "final.to_sql(\n",
    "    'Reviews',       # table name inside SQLite\n",
    "    conn,            # database connection\n",
    "    schema=None,     \n",
    "    if_exists='replace'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29c846dc-b5d1-4cbe-89a8-c15dc26dc3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364171, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>138706</td>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>939340800</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "      <td>b'witti littl book make son laugh loud recit c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138688</td>\n",
       "      <td>150506</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A2IW4PEEKO2R0U</td>\n",
       "      <td>Tracy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1194739200</td>\n",
       "      <td>Love the book, miss the hard cover version</td>\n",
       "      <td>I grew up reading these Sendak books, and watc...</td>\n",
       "      <td>b'grew read sendak book watch realli rosi movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138689</td>\n",
       "      <td>150507</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A1S4A3IQ2MU7V4</td>\n",
       "      <td>sally sue \"sally sue\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1191456000</td>\n",
       "      <td>chicken soup with rice months</td>\n",
       "      <td>This is a fun way for children to learn their ...</td>\n",
       "      <td>b'fun way children learn month year learn poem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138690</td>\n",
       "      <td>150508</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AZGXZ2UUK6X</td>\n",
       "      <td>Catherine Hallberg \"(Kate)\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1076025600</td>\n",
       "      <td>a good swingy rhythm for reading aloud</td>\n",
       "      <td>This is a great little book to read aloud- it ...</td>\n",
       "      <td>b'great littl book read nice rhythm well good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>138691</td>\n",
       "      <td>150509</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A3CMRKGE0P909G</td>\n",
       "      <td>Teresa</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>1018396800</td>\n",
       "      <td>A great way to learn the months</td>\n",
       "      <td>This is a book of poetry about the months of t...</td>\n",
       "      <td>b'book poetri month year goe month cute littl ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index      Id   ProductId          UserId                  ProfileName  \\\n",
       "0  138706  150524  0006641040   ACITT7DI6IDDL              shari zychinski   \n",
       "1  138688  150506  0006641040  A2IW4PEEKO2R0U                        Tracy   \n",
       "2  138689  150507  0006641040  A1S4A3IQ2MU7V4        sally sue \"sally sue\"   \n",
       "3  138690  150508  0006641040     AZGXZ2UUK6X  Catherine Hallberg \"(Kate)\"   \n",
       "4  138691  150509  0006641040  A3CMRKGE0P909G                       Teresa   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "0                     0                       0  positive   939340800   \n",
       "1                     1                       1  positive  1194739200   \n",
       "2                     1                       1  positive  1191456000   \n",
       "3                     1                       1  positive  1076025600   \n",
       "4                     3                       4  positive  1018396800   \n",
       "\n",
       "                                      Summary  \\\n",
       "0                   EVERY book is educational   \n",
       "1  Love the book, miss the hard cover version   \n",
       "2               chicken soup with rice months   \n",
       "3      a good swingy rhythm for reading aloud   \n",
       "4             A great way to learn the months   \n",
       "\n",
       "                                                Text  \\\n",
       "0  this witty little book makes my son laugh at l...   \n",
       "1  I grew up reading these Sendak books, and watc...   \n",
       "2  This is a fun way for children to learn their ...   \n",
       "3  This is a great little book to read aloud- it ...   \n",
       "4  This is a book of poetry about the months of t...   \n",
       "\n",
       "                                         CleanedText  \n",
       "0  b'witti littl book make son laugh loud recit c...  \n",
       "1  b'grew read sendak book watch realli rosi movi...  \n",
       "2  b'fun way children learn month year learn poem...  \n",
       "3  b'great littl book read nice rhythm well good ...  \n",
       "4  b'book poetri month year goe month cute littl ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score !=3\"\"\", conn)\n",
    "print(new_data.shape)\n",
    "new_data.head()  # Fixed: Added a new line and proper variable name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb12411d-b25a-43e3-91c5-3c5c3a430888",
   "metadata": {},
   "source": [
    "<h2>BIGRAM ,N-GRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83d85879-4573-4c28-80bf-ed16c47a9248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Positive Words :  [(b'like', 139429), (b'tast', 129047), (b'good', 112766), (b'flavor', 109624), (b'love', 107357), (b'use', 103888), (b'great', 103870), (b'one', 96726), (b'product', 91033), (b'tri', 86791), (b'tea', 83888), (b'coffe', 78814), (b'make', 75107), (b'get', 72125), (b'food', 64802), (b'would', 55568), (b'time', 55264), (b'buy', 54198), (b'realli', 52715), (b'eat', 52004)]\n",
      "Most Common Negative Words :  [(b'tast', 34585), (b'like', 32330), (b'product', 28218), (b'one', 20569), (b'flavor', 19575), (b'would', 17972), (b'tri', 17753), (b'use', 15302), (b'good', 15041), (b'coffe', 14716), (b'get', 13786), (b'buy', 13752), (b'order', 12871), (b'food', 12754), (b'dont', 11877), (b'tea', 11665), (b'even', 11085), (b'box', 10844), (b'amazon', 10073), (b'make', 9840)]\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Frequency Distribution of Words ----------------\n",
    "\n",
    "# Create frequency dictionary for words in positive reviews\n",
    "# Counts how many times each stemmed word appears\n",
    "freq_dist_positive = nltk.FreqDist(all_positive_words)      \n",
    "\n",
    "# Create frequency dictionary for words in negative reviews\n",
    "freq_dist_negative = nltk.FreqDist(all_negative_words)\n",
    "\n",
    "# Display 20 most common words in positive reviews\n",
    "# Output will show [(word, frequency), ...]\n",
    "print(\"Most Common Positive Words : \", freq_dist_positive.most_common(20))\n",
    "\n",
    "# Display 20 most common words in negative reviews\n",
    "print(\"Most Common Negative Words : \", freq_dist_negative.most_common(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c073af8-3a51-464e-a80f-b9ef3c009c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2910192)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bi-gram, tri-gram and n-gram\n",
    "\n",
    "# Note: Do NOT remove stop-words like \"not\" before n-gram generation\n",
    "# Because phrases like \"not good\", \"not happy\", \"not recommended\" are sentiment-important.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer for:\n",
    "# unigram (single words) + bigram (two-word combinations)\n",
    "# Example unigrams: good, product, bad, quality\n",
    "# Example bigrams: very good, not recommend, worth buying\n",
    "count_vect = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Fit the vectorizer on the cleaned text data and convert text to numerical feature vectors\n",
    "# final['Text'] contains all cleaned review text\n",
    "final_bigram_counts = count_vect.fit_transform(final['Text'].values)\n",
    "\n",
    "# Check the size of the generated matrix (documents x features)\n",
    "# Example output: (50000, 300000) → meaning:\n",
    "# 50,000 reviews, and 300,000 unique unigrams+bigrams learned\n",
    "final_bigram_counts.get_shape()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81712925-2409-4c9a-bb2c-bba4d1c0227f",
   "metadata": {},
   "source": [
    "<h2> TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "636ae8e7-5bcd-478b-a998-42eaf44869d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (364171, 2910192)\n",
      "Total features (unigrams + bigrams): 2910192\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TF-IDF Vectorizer\n",
    "# ngram_range=(1,2) means: include unigrams (1-word) and bigrams (2-word phrases)\n",
    "# This helps capture phrases like \"not good\", \"very bad\", etc.\n",
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Fit the vectorizer on the text data and convert text to TF-IDF features\n",
    "# final['Text'] → contains all the review sentences\n",
    "# .fit_transform() learns vocabulary + computes TF-IDF scores\n",
    "final_tf_idf = tf_idf_vect.fit_transform(final['Text'].values)\n",
    "\n",
    "# Show shape of TF-IDF matrix\n",
    "# Example: (50000, 320000)\n",
    "# → 50000 rows (documents/reviews)\n",
    "# → 320000 columns (unique unigrams + bigrams as features)\n",
    "print(\"TF-IDF matrix shape:\", final_tf_idf.shape)\n",
    "\n",
    "# Extract all feature (word) names from the TF-IDF vectorizer\n",
    "features = tf_idf_vect.get_feature_names_out()\n",
    "\n",
    "# Print total number of features (vocabulary size)\n",
    "# These features are words + 2-word phrases found in dataset\n",
    "print(\"Total features (unigrams + bigrams):\", len(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11ef422d-84f3-468e-997b-6b87b08e2ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ales until' 'ales ve' 'ales would' 'ales you' 'alessandra'\n",
      " 'alessandra ambrosia' 'alessi' 'alessi added' 'alessi also' 'alessi and']\n",
      "(2910192,)\n"
     ]
    }
   ],
   "source": [
    "# Print 10 feature names starting from index 100000\n",
    "# Slicing format → features[start : end]\n",
    "# This helps check random vocab words/bigrams created by TF-IDF\n",
    "\n",
    "print(features[100000:100010])\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb2d5ff7-717d-4107-859f-be7cb89f22f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Print TF-IDF values of the 4th review (row index = 3)\n",
    "# final_tf_idf[3,:] → select row 3 (entire vector for that document)\n",
    "# .toarray() → convert sparse matrix row into normal dense array\n",
    "# [0] → extract the actual 1-D vector from nested array\n",
    "\n",
    "print(final_tf_idf[3, :].toarray()[0])\n",
    "#what it will do\n",
    "\n",
    "\n",
    "# Length = number of TF-IDF features (ex: 320,000)\n",
    "# Mostly 0 values → word not present in this review\n",
    "# Non-zero values → TF-IDF weight for words actually present in the review\n",
    "\n",
    "# 0.45 = weight of some word/bigram in this review\n",
    "# 0.79 = weight of another word/bigram\n",
    "# Zeros mean the review did not contain those words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae6ab7d9-03fe-4545-b841-2cad17a4edcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>these sendak</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paperbacks seem</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rosie movie</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the paperbacks</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pages open</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sendak books</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cover version</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>incorporates them</td>\n",
       "      <td>0.168074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>paperbacks</td>\n",
       "      <td>0.168074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>really rosie</td>\n",
       "      <td>0.168074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hard cover</td>\n",
       "      <td>0.164269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>however miss</td>\n",
       "      <td>0.164269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>seem kind</td>\n",
       "      <td>0.161317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>up reading</td>\n",
       "      <td>0.156867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>that incorporates</td>\n",
       "      <td>0.155100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the pages</td>\n",
       "      <td>0.149737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sendak</td>\n",
       "      <td>0.149737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>of flimsy</td>\n",
       "      <td>0.146786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rosie</td>\n",
       "      <td>0.146786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>two hands</td>\n",
       "      <td>0.145130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>movie that</td>\n",
       "      <td>0.144374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>reading these</td>\n",
       "      <td>0.137184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>too do</td>\n",
       "      <td>0.134491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>incorporates</td>\n",
       "      <td>0.134147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>flimsy and</td>\n",
       "      <td>0.132254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature     tfidf\n",
       "0        these sendak  0.173437\n",
       "1     paperbacks seem  0.173437\n",
       "2         rosie movie  0.173437\n",
       "3      the paperbacks  0.173437\n",
       "4          pages open  0.173437\n",
       "5        sendak books  0.173437\n",
       "6       cover version  0.173437\n",
       "7   incorporates them  0.168074\n",
       "8          paperbacks  0.168074\n",
       "9        really rosie  0.168074\n",
       "10         hard cover  0.164269\n",
       "11       however miss  0.164269\n",
       "12          seem kind  0.161317\n",
       "13         up reading  0.156867\n",
       "14  that incorporates  0.155100\n",
       "15          the pages  0.149737\n",
       "16             sendak  0.149737\n",
       "17          of flimsy  0.146786\n",
       "18              rosie  0.146786\n",
       "19          two hands  0.145130\n",
       "20         movie that  0.144374\n",
       "21      reading these  0.137184\n",
       "22             too do  0.134491\n",
       "23       incorporates  0.134147\n",
       "24         flimsy and  0.132254"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source reference (good practice to credit useful code)\n",
    "# https://buhrmann.github.io/tfidf-analysis.html \n",
    "\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    \"\"\"\n",
    "    Return the top `top_n` TF-IDF features in a document.\n",
    "    \n",
    "    Parameters:\n",
    "    row      → TF-IDF vector for one document (as a 1D array)\n",
    "    features → List of feature names (words + bigrams)\n",
    "    top_n    → Number of top features to return (default = 25)\n",
    "\n",
    "    Output:\n",
    "    A DataFrame containing top words and their TF-IDF scores\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get indices of top TF-IDF values\n",
    "    # np.argsort(row) → sorts values, returning indices\n",
    "    # [::-1]          → reverse to get highest values first\n",
    "    # [:top_n]        → take only the first top_n indices\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    \n",
    "    # Create list of (word, tfidf_score) pairs for those top indices\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    \n",
    "    # Convert to dataframe for better readability\n",
    "    df = pd.DataFrame(top_feats, columns=['feature', 'tfidf'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Get top 25 TF-IDF features for the review at index 1\n",
    "top_tfidf = top_tfidf_feats(final_tf_idf[1,:].toarray()[0], features, 25)# Get top 25 most important words/bigrams (highest TF-IDF scores)\n",
    "                                                                         # for the review at index 1 (second review in dataset)\n",
    "\n",
    "# Show the result\n",
    "top_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7489f5d-9999-4078-a4c6-d61dbe8d012c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tfidf.shape\t#Meaning\n",
    "                            #25 rows\ttop 25 words/bigrams for that review\n",
    "                          # 2 columns\tfeature (word) and tfidf (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db47071f-27c2-46af-a557-ecc0af6c6aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gensim) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gensim) (1.15.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gensim) (7.4.2)\n",
      "Requirement already satisfied: wrapt in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ✅ Import necessary libraries\n",
    "# Word2Vec & KeyedVectors are tools for handling word embeddings (word vectors)\n",
    "!pip install gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle   # Used later for saving/loading Python objects (like dict of word vectors)\n",
    "\n",
    "# ✅ NOTE: We are using Google's pre-trained Word2Vec model\n",
    "# - This model was trained on Google News dataset (~100 billion words)\n",
    "# - File size ~1.9GB (.bin format) → after loading, uses ~9GB RAM in memory\n",
    "# - Therefore, run this only if your system has at least 12GB RAM\n",
    "# Download link (Google's public release):\n",
    "# https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "# ❗ Final expected file name: GoogleNews-vectors-negative300.bin\n",
    "# \"300\" = each word is represented as a 300-dimensional numerical vector\n",
    "\n",
    "# ✅ Load the pre-trained model\n",
    "# binary=True → because it is a .bin file (binary format)\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "# ✅ After loading, you can use:\n",
    "# model['king']  → gives 300-dimension vector for \"king\"\n",
    "# model.most_similar('king') → finds similar words based on meaning\n",
    "# model.similarity('king','queen') → computes similarity score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1fa5564-ca1e-46f7-8704-8288dd101ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
       "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
       "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
       "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
       "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
       "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
       "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
       "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
       "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
       "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
       "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
       "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
       "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
       "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
       "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
       "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
       "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
       "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
       "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
       "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
       "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
       "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
       "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
       "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
       "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
       "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
       "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
       "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
       "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
       "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
       "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
       "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
       "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
       "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
       "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
       "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
       "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
       "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
       "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
       "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
       "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
       "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
       "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
       "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
       "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
       "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
       "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
       "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
       "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
       "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
       "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
       "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
       "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
       "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
       "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
       "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
       "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
       "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
       "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
       "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
       "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
       "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
       "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
       "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
       "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
       "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
       "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
       "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
       "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
       "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
       "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
       "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
       "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
       "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
       "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['computer']\n",
    "#same output with     model.get_vector('computer')\n",
    "#or                    vector = model['computer']\n",
    "#print(vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82a2310a-9a7a-42cc-b8d4-948995a7162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(model.vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2f0e801-c9ba-448f-afad-f496396d3719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.76640123)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('woman','man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "422cedc9-420b-447d-add7-c675124d6e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.7664012908935547),\n",
       " ('girl', 0.7494640946388245),\n",
       " ('teenage_girl', 0.7336829900741577),\n",
       " ('teenager', 0.631708562374115),\n",
       " ('lady', 0.6288785934448242),\n",
       " ('teenaged_girl', 0.614178478717804),\n",
       " ('mother', 0.6076306104660034),\n",
       " ('policewoman', 0.6069462299346924),\n",
       " ('boy', 0.5975908637046814),\n",
       " ('Woman', 0.5770983099937439)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08eb6f32-600d-41de-9e72-d206c33aed87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('delicious', 0.8730388283729553),\n",
       " ('scrumptious', 0.8007041215896606),\n",
       " ('yummy', 0.7856924533843994),\n",
       " ('flavorful', 0.7420164346694946),\n",
       " ('delectable', 0.7385422587394714),\n",
       " ('juicy_flavorful', 0.7114803791046143),\n",
       " ('appetizing', 0.7017217874526978),\n",
       " ('crunchy_salty', 0.7012301087379456),\n",
       " ('flavourful', 0.6912213563919067),\n",
       " ('flavoursome', 0.6857702732086182)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('tasty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "745c97aa-150a-4798-be8e-266c813a2c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.44035056)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('tasty','tast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "90c1eb94-894a-414d-b4e4-a748b10871fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this witty little book makes my son laugh at loud. i recite it in the car as we're driving along and he always can sing the refrain. he's learned about whales, India, drooping roses:  i love all the new words this book  introduces and the silliness of it all.  this is a classic book i am  willing to bet my son will STILL be able to recite from memory when he is  in college\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "# ✅ Import gensim Word2Vec library\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "i = 0   # ✅ A simple counter, not necessary for logic, just used for reference in notebook\n",
    "list_of_sent = []   # ✅ This will finally store ALL sentences after cleaning + tokenizing\n",
    "                    #    Structure will become like:\n",
    "                    #    [\n",
    "                    #       ['this','is','first','sentence'],\n",
    "                    #       ['machine','learning','is','fun'],\n",
    "                    #       ...\n",
    "                    #    ]\n",
    "\n",
    "# ✅ Loop through every text entry in the DataFrame column named \"Text\"\n",
    "for sent in final['Text'].values:\n",
    "\n",
    "    filtered_sentence = []   # ✅ Temporary list to hold cleaned words for the current sentence\n",
    "    \n",
    "    # ✅ Remove HTML tags like <br>, <p>, <div> etc.\n",
    "    # Example: \"<p>Hello</p>\" → \"Hello\"\n",
    "    sent = cleanhtml(sent)\n",
    "\n",
    "    i = i + 1  # ✅ Increase counter (not used for logic, just debugging or print progress)\n",
    "\n",
    "    # ✅ Split sentence into raw tokens by whitespace\n",
    "    # Example: \"Hello world!!\" → [\"Hello\",\"world!!\"]\n",
    "    for w in sent.split():\n",
    "\n",
    "        # ✅ Clean punctuation from each word using custom cleanpunc() function\n",
    "        # Example: \"world!!\" → \"world\"\n",
    "        # If multiple tokens appear after cleaning, we split again\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "\n",
    "            # ✅ Keep only pure alphabetic tokens\n",
    "            # Means remove anything like \"123\", \"$$\", \"hello123\", etc.\n",
    "            if cleaned_words.isalpha():\n",
    "\n",
    "                # ✅ Convert word to lowercase\n",
    "                # \"Hello\" → \"hello\"\n",
    "                filtered_sentence.append(cleaned_words.lower())\n",
    "\n",
    "            else:\n",
    "                # ✅ If word contains digits or symbols, skip it\n",
    "                continue\n",
    "\n",
    "    # ✅ Store the cleaned + tokenized sentence into master list\n",
    "    list_of_sent.append(filtered_sentence)\n",
    "\n",
    "# ✅ Print the first original text for comparison\n",
    "print(final['Text'].values[0])\n",
    "\n",
    "# ✅ Separator for readability\n",
    "print(\"*\" * 10)\n",
    "print(list_of_sent[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e453e51-d1f8-4e42-b879-871e711c5fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'witty', 'little', 'book', 'makes', 'my', 'son', 'laugh', 'at', 'loud', 'i', 'recite', 'it', 'in', 'the', 'car', 'as', 'were', 'driving', 'along', 'and', 'he', 'always', 'can', 'sing', 'the', 'refrain', 'hes', 'learned', 'about', 'whales', 'india', 'drooping', 'i', 'love', 'all', 'the', 'new', 'words', 'this', 'book', 'introduces', 'and', 'the', 'silliness', 'of', 'it', 'all', 'this', 'is', 'a', 'classic', 'book', 'i', 'am', 'willing', 'to', 'bet', 'my', 'son', 'will', 'still', 'be', 'able', 'to', 'recite', 'from', 'memory', 'when', 'he', 'is', 'in', 'college']\n"
     ]
    }
   ],
   "source": [
    "print(list_of_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "52f43c64-fb7e-4605-ab20-a6cfa48e6744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Word2Vec Training Completed Successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ✅ Train Word2Vec model on your processed sentences\n",
    "w2v_model = Word2Vec(\n",
    "    list_of_sent,     # ✅ Input: List of tokenized sentences\n",
    "    vector_size=100,  # ✅ Word embedding dimension (100 features per word vector)\n",
    "    window=5,         # ✅ Words within distance 5 are considered context (around each word)\n",
    "    min_count=2,      # ✅ Ignore words that appear less than 2 times (removes noise)\n",
    "    workers=4         # ✅ Number of CPU cores to train faster (parallel training)\n",
    ")\n",
    "\n",
    "print(\"✅ Word2Vec Training Completed Successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c1c6c9f9-faa2-4cc4-b884-5833a97d8b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56874\n"
     ]
    }
   ],
   "source": [
    "# ✅ Extract all vocabulary words learned by the Word2Vec model\n",
    "# wv.key_to_index is a dictionary where:\n",
    "#   key   = word\n",
    "#   value = index (position in vocabulary)\n",
    "words = list(w2v_model.wv.key_to_index.keys())\n",
    "\n",
    "# ✅ Print number of unique words in the vocabulary\n",
    "# This tells how many distinct tokens Word2Vec learned embeddings for\n",
    "print(len(words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39476f41-22b8-4966-a058-2d70512ff135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'i', 'and', 'a', 'to', 'it', 'of', 'is', 'this', 'in', 'for', 'my', 'that', 'with', 'but', 'have', 'you', 'not', 'was', 'are']\n"
     ]
    }
   ],
   "source": [
    "print(words[:20])  # print first 20 words in vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6d51f556-1a99-47b4-84b9-e7278d2b62ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tastey', 0.8684049844741821),\n",
       " ('yummy', 0.8059287667274475),\n",
       " ('delicious', 0.7808747291564941),\n",
       " ('satisfying', 0.7790810465812683),\n",
       " ('flavorful', 0.7752999067306519),\n",
       " ('tasteful', 0.7307008504867554),\n",
       " ('filling', 0.7190523147583008),\n",
       " ('delish', 0.6836432814598083),\n",
       " ('addicting', 0.676529049873352),\n",
       " ('versatile', 0.6743167042732239)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('tasty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "81f9da13-3429-47e1-abb6-92178c06b774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dislike', 0.6002639532089233),\n",
       " ('resemble', 0.5835440754890442),\n",
       " ('prefer', 0.559781014919281),\n",
       " ('love', 0.5407053232192993),\n",
       " ('enjoy', 0.5397675633430481),\n",
       " ('weird', 0.5331843495368958),\n",
       " ('gross', 0.5220300555229187),\n",
       " ('overpower', 0.5217082500457764),\n",
       " ('mean', 0.5202308893203735),\n",
       " ('fake', 0.5133921504020691)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "91e6e62c-028e-426a-b03d-8fe390a280fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1442686\n",
      "activity great\n"
     ]
    }
   ],
   "source": [
    "# ✅ Get the list of all words (features) created by CountVectorizer\n",
    "#   This is the vocabulary that CountVectorizer learned from corpus\n",
    "count_vect_feat = count_vect.get_feature_names_out()\n",
    "\n",
    "# ✅ Find the index/position of the word \"like\" in the BoW vocabulary\n",
    "#   BoW assigns every word a numeric index\n",
    "word_index = list(count_vect_feat).index('like')\n",
    "\n",
    "# ✅ Print the index of the word \"like\"\n",
    "print(word_index)\n",
    "\n",
    "# ✅ Print the word at a specific vocabulary index (example: index 64055)\n",
    "#   This helps verify which word a particular BoW index corresponds to\n",
    "print(count_vect_feat[64055])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ae3a4d2f-94fc-4daa-b597-7c3b6bb9785a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2910192,)\n"
     ]
    }
   ],
   "source": [
    "print(count_vect_feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5939772-28af-40d8-a4da-d39eb89e306c",
   "metadata": {},
   "source": [
    "<h2> Average W2V,TF-IDF W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e28292a4-3d01-49e8-ae4b-64ec797a89e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364171\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# ✅ This list will store the average Word2Vec vector for each sentence/review\n",
    "sent_vectors = []  \n",
    "\n",
    "# ✅ Loop through each tokenized sentence in your corpus\n",
    "for sent in list_of_sent:  \n",
    "    \n",
    "    # ✅ Initialize a zero vector (size = 50 because Word2Vec vector dimension = 50)\n",
    "    # This will accumulate the sum of word vectors\n",
    "    sent_vec = np.zeros(100)  \n",
    "    \n",
    "    cnt_words = 0  # ✅ Counter to count only valid words (words present in vocab)\n",
    "\n",
    "    # ✅ Loop through every word in the sentence\n",
    "    for word in sent:  \n",
    "        try:\n",
    "            # ✅ Fetch Word2Vec vector for word\n",
    "            # w2v_model.wv[word] gives the embedding of that word\n",
    "            vec = w2v_model.wv[word]  \n",
    "\n",
    "            # ✅ Add this vector to the sentence vector\n",
    "            sent_vec += vec  \n",
    "\n",
    "            # ✅ Increase valid word count\n",
    "            cnt_words += 1  \n",
    "\n",
    "        except:\n",
    "            # ❌ If word not in vocabulary, skip it\n",
    "            pass  \n",
    "\n",
    "    # ✅ Average the sum of vectors by dividing by number of valid words\n",
    "    # This gives sentence embedding instead of raw sum\n",
    "    if cnt_words > 0:\n",
    "        sent_vec /= cnt_words  \n",
    "    \n",
    "    # ✅ Store the final sentence vector in list\n",
    "    sent_vectors.append(sent_vec)\n",
    "\n",
    "# ✅ Print number of sentence vectors\n",
    "print(len(sent_vectors))\n",
    "\n",
    "# ✅ Print length of a single sentence vector (should be 100)\n",
    "print(len(sent_vectors[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a73fb-8ed7-453b-a339-6d1473fa0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Get all words learned by TF-IDF (vocabulary)\n",
    "tfidf_feat = tf_idf_vect.get_feature_names_out()\n",
    "\n",
    "# ✅ This list will store the final sentence vectors\n",
    "tf_idf_sent_vectors = []\n",
    "\n",
    "row = 0  # ✅ Row index — each row of TF-IDF belongs to one sentence\n",
    "\n",
    "# ✅ Go through each sentence in the dataset\n",
    "for sent in list_of_sent:\n",
    "\n",
    "    # ✅ Start with a zero vector (100 dimensions = Word2Vec vector size)\n",
    "    sent_vec = np.zeros(100)\n",
    "\n",
    "    weight_sum = 0  # ✅ To store total TF-IDF value of all valid words in the sentence\n",
    "\n",
    "    # ✅ Go through each word of the sentence\n",
    "    for word in sent:\n",
    "        try:\n",
    "            # ✅ Get the Word2Vec vector of the current word\n",
    "            vec = w2v_model.wv[word]\n",
    "\n",
    "            # ✅ Get TF-IDF value of this word for this sentence\n",
    "            # tfidf_feat.tolist().index(word) → find column number of this word\n",
    "            tf_idf = final_tf_idf[row, tfidf_feat.tolist().index(word)]\n",
    "\n",
    "            # ✅ Add weighted vector (Word2Vec vector × TF-IDF weight)\n",
    "            sent_vec += (vec * tf_idf)\n",
    "\n",
    "            # ✅ Add TF-IDF weight to total weight\n",
    "            weight_sum += tf_idf\n",
    "\n",
    "        except:\n",
    "            # ❌ If word not found in Word2Vec OR not found in TF-IDF → skip it\n",
    "            pass\n",
    "\n",
    "    # ✅ Divide by total TF-IDF weight = final weighted average vector\n",
    "    if weight_sum != 0:\n",
    "        sent_vec /= weight_sum\n",
    "\n",
    "    # ✅ Add this sentence vector to result list\n",
    "    tf_idf_sent_vectors.append(sent_vec)\n",
    "\n",
    "    # ✅ Move to next row for next sentence\n",
    "    row += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b631d5f-92ee-4e1a-8159-d616d17dd3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tf_idf_sent_vectors))     # number of sentences\n",
    "print(len(tf_idf_sent_vectors[0]))  # should be 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15d495-035d-4f09-8656-f443b9150f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
